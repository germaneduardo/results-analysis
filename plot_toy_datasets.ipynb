{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# to Hive DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn==0.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.datasets import make_classification, make_circles, make_moons, make_blobs\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "ir = 100\n",
    "n_samples = 10000\n",
    "\n",
    "def to_hive_safely(df, table):\n",
    "    df=df.withColumn(\"idn\", F.col(\"idn\").cast(\"int\"))\n",
    "    df=df.withColumn(\"label\", F.col(\"label\").cast(\"int\"))\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"feature_1\", \"feature_2\"],\n",
    "        outputCol=\"features_no_scale\")\n",
    "    df = assembler.transform(df)\n",
    "    scaler = StandardScaler(inputCol=\"features_no_scale\", outputCol=\"features\",\n",
    "                        withStd=True, withMean=True)\n",
    "    scalerModel = scaler.fit(df)\n",
    "    scaledData = scalerModel.transform(df)\n",
    "    scaledData.write.mode(\"overwrite\").saveAsTable(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= make_classification(n_samples = n_samples*10, n_features=2, n_informative= 1, n_redundant=0, n_clusters_per_class= 1,  random_state=10,  class_sep=0.5)\n",
    "X, y = make_imbalance(X, y,sampling_strategy={0: int(n_samples/(ir+1)), 1: int((ir)*(n_samples/(ir+1)))},\n",
    "                              random_state=42)\n",
    "data_ir_100 = pd.DataFrame(X, columns = ['feature_1', 'feature_2'])\n",
    "data_ir_100['label'] = y\n",
    "data_ir_100.reset_index(inplace=True)\n",
    "data_ir_100.rename(columns={'index': 'idn'}, inplace=True)\n",
    "df=spark.createDataFrame(data_ir_100)\n",
    "to_hive_safely(df, \"dummy_data_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= make_circles(n_samples = n_samples*10,noise=0.05, random_state=42,shuffle=True)\n",
    "X, y = make_imbalance(X, y,sampling_strategy={0: int(n_samples/(ir+1)), 1: int((ir)*(n_samples/(ir+1)))},\n",
    "                              random_state=42)\n",
    "data_ir_100 = pd.DataFrame(X, columns = ['feature_1', 'feature_2'])\n",
    "data_ir_100['label'] = y\n",
    "data_ir_100.reset_index(inplace=True)\n",
    "data_ir_100.rename(columns={'index': 'idn'}, inplace = True)\n",
    "df=spark.createDataFrame(data_ir_100)\n",
    "to_hive_safely(df, \"dummy_data_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= make_moons(n_samples = n_samples*10,noise=0.2, random_state=42, shuffle=True)\n",
    "X, y = make_imbalance(X, y,sampling_strategy={0: int(n_samples/(ir+1)), 1: int((ir)*(n_samples/(ir+1)))},\n",
    "                              random_state=42)\n",
    "data_ir_100 = pd.DataFrame(X, columns = ['feature_1', 'feature_2'])\n",
    "data_ir_100['label'] = y\n",
    "data_ir_100.reset_index(inplace=True)\n",
    "data_ir_100.rename(columns={'index': 'idn'}, inplace = True)\n",
    "df=spark.createDataFrame(data_ir_100)\n",
    "to_hive_safely(df, \"dummy_data_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y= make_blobs(n_samples = n_samples*10, random_state=42, shuffle=True, centers=2, cluster_std=3)\n",
    "X, y = make_imbalance(X, y,sampling_strategy={0: int(n_samples/(ir+1)), 1: int((ir)*(n_samples/(ir+1)))},\n",
    "                              random_state=42)\n",
    "data_ir_100 = pd.DataFrame(X, columns = ['feature_1', 'feature_2'])\n",
    "data_ir_100['label'] = y\n",
    "data_ir_100.reset_index(inplace=True)\n",
    "data_ir_100.rename(columns={'index': 'idn'}, inplace = True)\n",
    "df=spark.createDataFrame(data_ir_100)\n",
    "to_hive_safely(df, \"dummy_data_v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make comparing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "sns.set_context(\"notebook\", font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = pathlib.Path(\"data/datasetExample\") \n",
    "results = pd.DataFrame()\n",
    "ds = []\n",
    "methods = []\n",
    "datasets_df = []\n",
    "\n",
    "for i in path.glob('**/*.csv'):\n",
    "    partial = pd.read_csv(i.absolute(), header=None)\n",
    "    partial.columns = ['idn', 'feature_1', 'feature_2', 'label', 'is_selected']\n",
    "    paths = i.parts[2].split(\"_\")\n",
    "    datasetid = \"_\".join(paths[0:3])\n",
    "    partial['dataset_id'] = datasetid\n",
    "\n",
    "    if datasetid not in ds:\n",
    "        print(\"first datasets\")\n",
    "        datasets_df.append(partial.drop(\"is_selected\", axis = 1))\n",
    "\n",
    "    partial['ands'] = int(paths[4])\n",
    "    partial['lsh'] = paths[3]\n",
    "    if paths[5]=='drop3':\n",
    "        #method = f\"LSH: {paths[3]} with {paths[4]} ANDS, IS: {'_'.join(paths[5:])}\"\n",
    "        partial['is'] = '_'.join(paths[5:])\n",
    "        method = f\"{paths[3]}\"\n",
    "        partial['is_ands'] = f\"{'_'.join(paths[5:])}-{paths[4]} ANDS\"\n",
    "    else:\n",
    "        #method = f\"LSH: {paths[3]} with {paths[4]} ANDS, IS: {paths[5]}\"\n",
    "        partial['is'] = paths[5]\n",
    "        method = f\"{paths[3]}\"\n",
    "        partial['is_ands'] = f\"{paths[5]}-{paths[4]} ANDS\"\n",
    "    #print(i.absolute(), datasetid, method)\n",
    "    partial['LSH'] = method\n",
    "    to_append = [results]       \n",
    "    to_append.append(partial[partial.is_selected==1].copy().drop(\"is_selected\", axis = 1))\n",
    "    results = pd.concat(to_append, ignore_index = True)\n",
    "    ds.append(datasetid)\n",
    "    methods.append(method)\n",
    "ds = list(set(ds))\n",
    "methods = list(set(methods))\n",
    "total_plots = len(ds)*len(methods)\n",
    "results= results[results.ands.isin([2,10])].copy().reset_index(drop = True)\n",
    "datasets_df = pd.concat(datasets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh_map = {'hyperplanes': 'RHF', 'projection':'DPF', 'hyperplanes-projection': 'RHF+DPF'}\n",
    "results.LSH = results.LSH.map(lsh_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots based on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(ds_p):\n",
    "    g = sns.FacetGrid(data = results[results.dataset_id==ds_p],\n",
    "                  hue = 'label', row = 'LSH', col='is_ands',\n",
    "                  col_order =col_order, row_order = row_order,\n",
    "                  margin_titles = True, despine=False, aspect=1.3, height=3.5)\n",
    "    \n",
    "    g.map_dataframe(sns.scatterplot,\n",
    "                x='feature_1', \n",
    "                y='feature_2', \n",
    "                alpha = 0.8,\n",
    "                marker = \"x\",\n",
    "                palette = 'bright', s=120)\n",
    "    \n",
    "    \"\"\"\n",
    "    for (col_val, row_val), ax in g.axes_dict.items():\n",
    "        #print(col_val, row_val)\n",
    "        ax.set_title(row_val, fontdict = {\"fontsize\": \"xx-large\"})\n",
    "        ax.set_facecolor(face_color[row_val])\n",
    "        mar_title = [c for c in ax.get_children() if type(c) == matplotlib.text.Annotation]\n",
    "        mar_title = [c for c in mar_title if \"LSH\" in c.get_text()]\n",
    "        if len(mar_title)>0:\n",
    "            mar_title = mar_title[0] \n",
    "            mar_title.set_fontsize(\"x-large\")\n",
    "            print(mar_title)\n",
    "    \n",
    "        sns.scatterplot(data= datasets_df[datasets_df['dataset_id']==ds_p], \n",
    "        x = 'feature_1', y = 'feature_2', hue = 'label' , alpha=0.05, ax = ax, legend = False,  marker=\"+\", palette='dark')\n",
    "    \"\"\"\n",
    "    for (col_val, row_val), ax in g.axes_dict.items():\n",
    "        #print(col_val, row_val)\n",
    "        d_referece = datasets_df[datasets_df['dataset_id']==ds_p].copy().label.value_counts()\n",
    "        res = results[(results.dataset_id==ds_p) & (results.LSH==col_val) & (results.is_ands==row_val)].label.value_counts()\n",
    "        ir_new = res.values[0]/res.values[1]\n",
    "        ir = d_referece.values[0]/d_referece.values[1]\n",
    "        diff = (ir-ir_new)/ir*100\n",
    "        reduction = f\"IR {ir_new:0.2F}. IR reduced in {diff:0.2F}%\"\n",
    "        mar_title = [c for c in ax.get_children() if type(c) == matplotlib.text.Annotation]\n",
    "        mar_title = [c for c in mar_title if \"LSH\" in c.get_text()]\n",
    "        xx = ax.get_xlim()\n",
    "        yy = ax.get_ylim()\n",
    "        ax.text(xx[1]*0.2, yy[0]*0.7, reduction, fontsize=16,  ha=\"center\", va=\"center\",bbox = {'edgecolor':\"black\", 'facecolor':\"white\", 'alpha' : 0.3}) \n",
    "            \n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "    g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f'report/{ds_p}.png', dpi = 300)\n",
    "    print(ds_p, \"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example(ds[ds.index('dummy_data_v2')])\n",
    "plot_example(ds[ds.index('dummy_data_v3')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plots based on methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(ds_p, is_ands, name_file):\n",
    "    cond_to_plot =  (results.dataset_id.isin(ds_p)) & (results.is_ands.isin(is_ands))\n",
    "    g = sns.FacetGrid(data = results[cond_to_plot],\n",
    "                  hue = 'label', row = 'LSH', col='is_ands',\n",
    "                  col_order = col_order, row_order = row_order,\n",
    "                  margin_titles = True, despine=False, aspect=1.3, height=3.5)\n",
    "    \n",
    "    g.map_dataframe(sns.scatterplot,\n",
    "                x='feature_1', \n",
    "                y='feature_2', \n",
    "                alpha = 0.8,\n",
    "                marker = \"x\",\n",
    "                palette = 'bright', s=120)\n",
    "    \n",
    "    ir_df = pd.DataFrame(columns = ['LSH', 'is', 'ir', 'red'])\n",
    "    idx = 0\n",
    "    for (col_val, row_val), ax in g.axes_dict.items():\n",
    "        d_referece = datasets_df[datasets_df['dataset_id']==ds_p[0]].copy().label.value_counts()\n",
    "        res = results[(results.dataset_id==ds_p[0]) & (results.LSH==col_val) & (results.is_ands==row_val)].label.value_counts()\n",
    "        ir_new = res.values[0]/res.values[1]\n",
    "        ir = d_referece.values[0]/d_referece.values[1]\n",
    "\n",
    "        diff = (ir-ir_new)/ir*100\n",
    "        reduction = f\"IR {ir_new:0.2F}. IR reduced in {diff:0.2F}%\"\n",
    "        reduction = f\"IR reduced in {diff:0.2F}%\"\n",
    "        mar_title = [c for c in ax.get_children() if type(c) == matplotlib.text.Annotation]\n",
    "        mar_title = [c for c in mar_title if \"LSH\" in c.get_text()]\n",
    "        xx = ax.get_xlim()\n",
    "        yy = ax.get_ylim()\n",
    "        ax.text(xx[1]*0.2, yy[0]*0.7, reduction, fontsize=16,  ha=\"center\", va=\"center\",bbox = {'edgecolor':\"black\", 'facecolor':\"white\", 'alpha' : 0.3})\n",
    "        ir_df.loc[idx, :] =  col_val,row_val,ir_new,diff\n",
    "        idx+=1\n",
    "            \n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "    g.fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(f'report/{name_file}.png', dpi = 300)\n",
    "    print(ds_p, \"saved\")\n",
    "    return (ir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"notebook\", font_scale=1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_color = {\"entropy-2 ANDS\": \"1.0\", \"entropy-10 ANDS\": \"1.0\",\n",
    "              \"drop3_one-2 ANDS\": \".9\", \"drop3_one-10 ANDS\":\".9\",\n",
    "              \"drop3_boundaries-2 ANDS\": \".75\", \"drop3_boundaries-10 ANDS\": \".75\"}\n",
    "\n",
    "face_color = {\"entropy-2 ANDS\": \"1.0\", \"entropy-10 ANDS\": \"1.0\",\n",
    "              \"drop3_one-2 ANDS\": \"1.0\", \"drop3_one-10 ANDS\":\"1.0\",\n",
    "              \"drop3_boundaries-2 ANDS\": \"1.0\", \"drop3_boundaries-10 ANDS\": \"1.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"entropy-2 ANDS\", \"entropy-10 ANDS\", ]\n",
    "ir_dif_1 = plot_example(['dummy_data_v3'], [c for c in results.is_ands.unique() if 'entropy' in c], 'entropy_1')\n",
    "ir_dif_2 = plot_example(['dummy_data_v2'], [c for c in results.is_ands.unique() if 'entropy' in c], 'entropy_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_dif_1['dataset'] = 'interleaving half circles'\n",
    "ir_dif_2['dataset'] = 'inner circles'\n",
    "ir_df = pd.concat([ir_dif_1,ir_dif_2], ignore_index=True)\n",
    "ir_df['ANDS'] = ir_df['is'].str.split(\"-\").apply(lambda x : int(x[-1].split(\"ANDS\")[0]))\n",
    "ir_df['IS'] = ir_df['is'].str.split(\"-\").apply(lambda x : x[0])\n",
    "ir_df['IR'] = pd.to_numeric(ir_df['ir'])\n",
    "sns.color_palette(\"bright\")\n",
    "#sns.relplot(data = ir_df, x='ANDS', hue= 'method', y = 'IR', kind = 'line', row= 'dataset', aspect=1, height=5)\n",
    "g = sns.FacetGrid(data = ir_df,\n",
    "                  hue = 'LSH', row = 'dataset',\n",
    "                  margin_titles = True, legend_out = False, \n",
    "                  despine=True, aspect=0.9, height=7)\n",
    "\n",
    "g.map_dataframe(sns.lineplot,\n",
    "            x='ANDS', \n",
    "            y='IR', linewidth = 4.5, linestyle = \"--\"), \n",
    "g.set_titles(row_template=\"{row_name}\")\n",
    "g.set_ylabels(\"IR\")\n",
    "g.set_xlabels(\"ANDS\")\n",
    "for ax in g.axes.ravel():\n",
    "    ax.legend()\n",
    "\n",
    "plt.savefig(f'report/entropy_ir.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"drop3_one-2 ANDS\", \"drop3_one-10 ANDS\",\n",
    "            \"drop3_boundaries-2 ANDS\", \"drop3_boundaries-10 ANDS\"]\n",
    "\n",
    "ir_dif_1 = plot_example(['dummy_data_v3'], [c for c in results.is_ands.unique() if 'entropy' not in c], 'drop3_1')\n",
    "ir_dif_2 = plot_example(['dummy_data_v2'], [c for c in results.is_ands.unique() if 'entropy' not in c], 'drop3_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_dif_1['dataset'] = 'interleaving half circles'\n",
    "ir_dif_2['dataset'] = 'inner circles'\n",
    "ir_df = pd.concat([ir_dif_1,ir_dif_2], ignore_index=True)\n",
    "ir_df['ANDS'] = ir_df['is'].str.split(\"-\").apply(lambda x : int(x[-1].split(\"ANDS\")[0]))\n",
    "ir_df['IS'] = ir_df['is'].str.split(\"-\").apply(lambda x : x[0])\n",
    "ir_df['IR'] = pd.to_numeric(ir_df['ir'])\n",
    "sns.color_palette(\"bright\")\n",
    "#sns.relplot(data = ir_df, x='ANDS', hue= 'method', y = 'IR', kind = 'line', row= 'dataset', aspect=1, height=5)\n",
    "g = sns.FacetGrid(data = ir_df,\n",
    "                  col = 'dataset',\n",
    "                  margin_titles = True, legend_out = True, \n",
    "                  despine=True, aspect=2, height=6)\n",
    "\n",
    "g.map_dataframe(sns.lineplot, hue = 'LSH',\n",
    "            x='ANDS', style = 'IS',\n",
    "            y='IR', linewidth = 3), \n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_ylabels(\"IR\")\n",
    "g.set_xlabels(\"ANDS\")\n",
    "for ax in g.axes.ravel():\n",
    "    ax.legend(fontsize = 'x-small', framealpha= 0.3,ncol = 2)\n",
    "\n",
    "\n",
    "plt.savefig(f'report/drop3_ir.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70bdedba4ab57e46c8363e9bcb2fadae69894a0ea07a1c5c3ed5763e090ee9b3"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('vonneumann': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
